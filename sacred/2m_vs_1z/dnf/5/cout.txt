[INFO 18:08:16] pymarl Running command 'my_main'
[INFO 18:08:16] pymarl Started run with ID "5"
[DEBUG 18:08:16] pymarl Starting Heartbeat
[DEBUG 18:08:16] my_main Started
[INFO 18:08:16] my_main Experiment Parameters:
[INFO 18:08:16] my_main 

{   'action_selector': 'epsilon_greedy',
    'agent': 'dnf_rnn',
    'agent_output_type': 'q',
    'asn_hidden_size': 32,
    'batch_size': 32,
    'batch_size_run': 2,
    'buffer_cpu_only': True,
    'buffer_size': 5000,
    'checkpoint_path': '',
    'core_agent_ratio': 0.7,
    'core_extractor_layer_norm': False,
    'core_extractor_type': 'svd',
    'core_extractor_use_orthogonal': False,
    'core_hidden_dim': 32,
    'cpu_inference': True,
    'critic_lr': 0.0005,
    'double_q': True,
    'enable_parallel_computing': False,
    'env': 'sc2',
    'env_args': {   'continuing_episode': False,
                    'debug': False,
                    'difficulty': '7',
                    'game_version': None,
                    'heuristic_ai': False,
                    'heuristic_rest': False,
                    'map_name': '2m_vs_1z',
                    'move_amount': 2,
                    'obs_all_health': True,
                    'obs_instead_of_state': False,
                    'obs_last_action': False,
                    'obs_own_health': True,
                    'obs_pathing_grid': False,
                    'obs_terrain_height': False,
                    'obs_timestep_number': False,
                    'replay_dir': '',
                    'replay_prefix': '',
                    'reward_death_value': 10,
                    'reward_defeat': 0,
                    'reward_negative_scale': 0.5,
                    'reward_only_positive': True,
                    'reward_scale': True,
                    'reward_scale_rate': 20,
                    'reward_sparse': False,
                    'reward_win': 200,
                    'seed': 638743387,
                    'state_last_action': True,
                    'state_timestep_number': False,
                    'step_mul': 8},
    'epsilon_anneal_time': 100000,
    'epsilon_finish': 0.05,
    'epsilon_start': 1.0,
    'evaluate': False,
    'gain': 0.01,
    'gamma': 0.99,
    'grad_norm_clip': 10,
    'hypernet_embed': 64,
    'label': 'default_label',
    'learner': 'nq_learner',
    'learner_log_interval': 10000,
    'load_step': 0,
    'local_results_path': 'results',
    'log_interval': 10000,
    'lr': 0.001,
    'mac': 'dnf_mac',
    'mixer': 'qmix',
    'mixing_embed_dim': 32,
    'name': 'dnf',
    'obs_agent_id': True,
    'obs_last_action': True,
    'optim_alpha': 0.99,
    'optim_eps': 1e-05,
    'optimizer': 'adam',
    'per_alpha': 0.6,
    'per_beta': 0.4,
    'q_lambda': False,
    'repeat_id': 1,
    'return_priority': False,
    'rnn_hidden_dim': 64,
    'run': 'default',
    'runner': 'parallel',
    'runner_log_interval': 10000,
    'save_model': True,
    'save_model_interval': 2000000,
    'save_replay': False,
    'seed': 638743387,
    't_max': 1000000,
    'target_update_interval': 200,
    'td_lambda': 0.6,
    'test_greedy': True,
    'test_interval': 10000,
    'test_nepisode': 32,
    'thread_num': 12,
    'use_cuda': True,
    'use_layer_norm': False,
    'use_orthogonal': False,
    'use_per': False,
    'use_tensorboard': True}

{'state_shape': 26, 'obs_shape': 16, 'n_actions': 7, 'n_agents': 2, 'n_enemies': 1, 'episode_limit': 150, 'n_normal_actions': 6, 'n_allies': 1, 'state_ally_feats_size': 4, 'state_enemy_feats_size': 4, 'obs_component': [4, (1, 6), (1, 5), 1], 'state_component': [8, 4, 14], 'map_type': 'marines'}
&&&&&&&&&&&&&&&&&&&&&& dnf_rnn 27.079K
&&&&&&&&&&&&&&&&&&&&&& svd 27.079K
dominators_idx: {}, followers_idx: {} tensor([0., 0.]) tensor([])
 &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& self.batch_device=cpu
Mixer Size: 
11.457K
[INFO 18:09:53] my_main Beginning training for 1000000 timesteps
